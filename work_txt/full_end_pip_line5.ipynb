{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# системные пакеты\n",
    "import os\n",
    "from glob import glob\n",
    "from pymystem3 import Mystem\n",
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from pymystem3 import Mystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# пакеты машинного обучения\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Получение локального пути приложения\n",
    "pp = os.getcwd()\n",
    "PATH_DD = pp.split('group_mlada')[0] + 'group_mlada/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Читаем тренировочные данные\n",
    "train_path = f'{PATH_DD}/data/train/train.parquet'\n",
    "train = pd.read_parquet(train_path, engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# читаем файл с описаниями\n",
    "text_and_bert_path = f'{PATH_DD}/data/train/text_and_bert.parquet'\n",
    "text_ful = pd.read_parquet(text_and_bert_path, engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Читаем файл с атрибутами\n",
    "attributes_path = f'{PATH_DD}/data/train/attributes.parquet'\n",
    "attributes = pd.read_parquet(attributes_path, engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_len_categories1 = Counter()\n",
    "stat_len_categories2 = Counter()\n",
    "stat_len_categories3 = Counter()\n",
    "stat_len_categories4 = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(attributes)):\n",
    "    dd = json.loads(attributes.iloc[i]['categories'])\n",
    "    stat_len_categories1[dd['1']] +=1\n",
    "    stat_len_categories2[dd['2']] +=1\n",
    "    stat_len_categories3[dd['3']] +=1\n",
    "    stat_len_categories4[dd['4']] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "characteristic = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(attributes)):\n",
    "    dd = json.loads(attributes.iloc[i]['characteristic_attributes_mapping'])\n",
    "    for key in dd:\n",
    "        characteristic[key] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dit_param(ar_param, litera:str) ->dict:\n",
    "    result = {}\n",
    "    anum = 0\n",
    "    for key, _ in ar_param.most_common():\n",
    "        result[key] = litera + str(anum)\n",
    "        anum += 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_characteristic = get_dit_param(characteristic, 'e')\n",
    "d_categories2 = get_dit_param(stat_len_categories2, 'b')\n",
    "d_categories3 = get_dit_param(stat_len_categories3, 'c')\n",
    "d_categories4 = get_dit_param(stat_len_categories4, 'd')\n",
    "d_all = {'d2': d_categories2,\n",
    "         'd3': d_categories3,\n",
    "         'd4': d_categories4,\n",
    "         'dc': d_characteristic}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_desc_obj(d_all, row):\n",
    "    '''\n",
    "    Получение описание объекта в кодах-словах\n",
    "    '''\n",
    "    r_d = []\n",
    "    dd = json.loads(row['categories'])\n",
    "    r_d.append( d_all['d2'][dd['2']])\n",
    "    r_d.append( d_all['d3'][dd['3']])\n",
    "    r_d.append( d_all['d4'][dd['4']])\n",
    "    for key in json.loads(row['characteristic_attributes_mapping']):\n",
    "        r_d.append(d_all['dc'][key])\n",
    "    return ' '.join(r_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_all_obj ={}\n",
    "all_v = []\n",
    "for i in range(0, len(attributes)):\n",
    "    v = attributes.iloc[i]['variantid']\n",
    "    all_v.append(str(v).strip())\n",
    "    desc_all_obj[str(v).strip()] = get_desc_obj(d_all, attributes.iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Распечатка строки описания объектов в кодах категорий\n",
    "def print_obj(desc_all_obj, nn: int):\n",
    "    k = 0\n",
    "    for key in desc_all_obj:\n",
    "        if k == nn:\n",
    "                print(key, desc_all_obj[key], sep=' ==> ')\n",
    "                break\n",
    "        k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "628056916 ==> b9 c52 d108 e978 e494 e305 e423 e239 e230 e297 e580 e257 e19 e9 e518 e4 e14 e255 e0 e1 e452 e298 e405 e380 e68 e700 e394 e463 e638 e241 e363 e236 e359 e419 e849 e412 e672 e268 e783 e244 e273 e289 e238 e252 e616 e2343 e2 e418 e443 e685 e403 e3 e6 e275 e69 e295\n",
      "628105125 ==> b4 c252 d1174 e4 e14 e100 e27 e0 e1 e63 e44 e182 e1780 e3 e1799 e6 e151 e2 e145 e5\n",
      "628139361 ==> b4 c2 d0 e8 e27 e0 e2\n"
     ]
    }
   ],
   "source": [
    "print_obj(desc_all_obj, 999)\n",
    "print_obj(desc_all_obj, 1000)\n",
    "print_obj(desc_all_obj, 1001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Mystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def get_diig(ss: str) -> str:\n",
    "    '''\n",
    "    удаление полностью чисел\n",
    "    '''\n",
    "    return re.sub(r'\\d', '', ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = []\n",
    "with open(PATH_DD + 'data/all_words_stat_stop.csv', 'r') as f:\n",
    "    for line in f:\n",
    "        word_s = line.strip().split('^')[0]\n",
    "        stop_words.append(word_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lem_words_three(ss: str, stop_words) -> list:\n",
    "    '''\n",
    "    Получение слов из строки\n",
    "    '''\n",
    "    ww= []\n",
    "    ss = ss.strip().lower()\n",
    "    ddel = ['*', ',', '-', '/', '_', '%', ';', ')', '(', '[', ']','+', '.', '..', ':', '°', '«', '»']\n",
    "    for i in ddel:\n",
    "        ss = ss.replace(i, ' ')\n",
    "    ssl = ss.split()\n",
    "    for word in ssl:\n",
    "        # print(word)\n",
    "        wwd = get_diig(word)\n",
    "        if len(wwd) > 1:\n",
    "            lemmas = m.lemmatize(wwd)\n",
    "            if lemmas[0] not in stop_words and len(lemmas[0]) > 1:\n",
    "                ww.append(lemmas[0])\n",
    "    return ww"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sequence_three(row, stop_words):\n",
    "    '''\n",
    "    Получение предложения описания объекта в кодах-словах\n",
    "    '''\n",
    "    all_word = []\n",
    "    ar = json.loads(row['characteristic_attributes_mapping'])\n",
    "    for key in ar:\n",
    "        if isinstance(ar[key], list):\n",
    "            # print(key)\n",
    "            for ww in ar[key]:\n",
    "                das = get_lem_words_three(ww, stop_words)\n",
    "                all_word = all_word + das\n",
    "            # print()\n",
    "    all_word = set(all_word)\n",
    "    return ' '.join(all_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2252569\n",
      "2252569\n"
     ]
    }
   ],
   "source": [
    "X_words_three = []\n",
    "pusto = 0\n",
    "Y_var_three = []\n",
    "for i in range(0, len(attributes)):\n",
    "    v = attributes.iloc[i]['variantid']\n",
    "    Y_var_three.append(str(v).strip())\n",
    "    sequence_three = get_sequence_three(attributes.iloc[i], stop_words)\n",
    "    if sequence_three:\n",
    "        X_words_three.append(sequence_three)\n",
    "    else:\n",
    "        X_words_three.append(['pusto'])\n",
    "        pusto += 1\n",
    "print(len(X_words_three))\n",
    "print(len(X_words_three))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PATH_DD + 'data/X_words_four.csv', 'w') as f:\n",
    "    for key, ssss in zip(Y_var_three, X_words_three):\n",
    "        f.write(key + '^' + ssss + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'семя желтый все муромец бренд август март июль февраль илья регион россия однолетник май теплица томат питание улица диетический'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_words_three[1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_all_obj4= {}\n",
    "for key, sens in zip(Y_var_three, X_words_three):\n",
    "    desc_all_obj4[key] = sens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X4=[]\n",
    "Y4=[]\n",
    "for row in train.iterrows():\n",
    "    if str(row[1]['variantid1']).strip() in desc_all_obj and str(row[1]['variantid2']).strip() in desc_all_obj:\n",
    "        # print(desc_all_obj[row[1]['variantid1']])\n",
    "        a = ''\n",
    "        b = ''\n",
    "        if type(desc_all_obj4[str(row[1]['variantid1'])]) == list:\n",
    "            a = 'pusto'\n",
    "        else: a = desc_all_obj4[str(row[1]['variantid1']).strip()]\n",
    "        if type(desc_all_obj4[str(row[1]['variantid2'])]) == list:\n",
    "            b = 'pusto'\n",
    "        else: b = desc_all_obj4[str(row[1]['variantid2']).strip()]\n",
    "        X4.append(desc_all_obj[str(row[1]['variantid1']).strip()] + ' ' + a + ' ' +\n",
    "                  desc_all_obj[str(row[1]['variantid2']).strip()] + ' ' + b)\n",
    "        Y4.append(row[1]['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(PATH_DD + 'data/Y5_X5_X_words_four.csv', 'w') as f:\n",
    "    for key, ssss in zip(Y4, X4):\n",
    "        f.write(str(key) + '^' + ssss + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Классификатор Random Forest\n",
    "def train_classifier(data, labels):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(data)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.2, random_state=42)\n",
    "    \n",
    "    random_forest_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "    random_forest_model.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred_rf = random_forest_model.predict(X_test)\n",
    "    print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
    "    precision = precision_score(y_test, y_pred_rf)\n",
    "    recall = recall_score(y_test, y_pred_rf)\n",
    "    auc = roc_auc_score(y_test, y_pred_rf)\n",
    "    # print(f'Accuracy: {accuracy:.4f}')\n",
    "    print(f'Precision: {precision:.4f}')\n",
    "    print(f'Recall: {recall:.4f}')\n",
    "    print(f'AUC: {auc:.4f}')\n",
    "    \n",
    "    return random_forest_model, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.7875432170608976\n",
      "Precision: 0.7761\n",
      "Recall: 0.7843\n",
      "AUC: 0.7874\n"
     ]
    }
   ],
   "source": [
    "random_forest_model4, vectorizer4 = train_classifier(X4, Y4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
